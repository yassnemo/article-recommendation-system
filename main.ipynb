{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [45 lines of output]\n",
      "      Compiling surprise/similarities.pyx because it changed.\n",
      "      Compiling surprise/prediction_algorithms/matrix_factorization.pyx because it changed.\n",
      "      Compiling surprise/prediction_algorithms/optimize_baselines.pyx because it changed.\n",
      "      Compiling surprise/prediction_algorithms/slope_one.pyx because it changed.\n",
      "      Compiling surprise/prediction_algorithms/co_clustering.pyx because it changed.\n",
      "      [1/5] Cythonizing surprise/prediction_algorithms/co_clustering.pyx\n",
      "      \n",
      "      Error compiling Cython file:\n",
      "      ------------------------------------------------------------\n",
      "      ...\n",
      "              self.avg_cltr_i = avg_cltr_i\n",
      "              self.avg_cocltr = avg_cocltr\n",
      "      \n",
      "              return self\n",
      "      \n",
      "          def compute_averages(self, np.ndarray[np.int_t] cltr_u,\n",
      "                                                   ^\n",
      "      ------------------------------------------------------------\n",
      "      \n",
      "      surprise\\prediction_algorithms\\co_clustering.pyx:157:45: Invalid type.\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "          ~~~~^^\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-5kf9h4ol\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-5kf9h4ol\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n",
      "          self.run_setup()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-5kf9h4ol\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "          ~~~~^^^^^^^^^^^^^^^^\n",
      "        File \"<string>\", line 116, in <module>\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-5kf9h4ol\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1154, in cythonize\n",
      "          cythonize_one(*args)\n",
      "          ~~~~~~~~~~~~~^^^^^^^\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-5kf9h4ol\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1321, in cythonize_one\n",
      "          raise CompileError(None, pyx_file)\n",
      "      Cython.Compiler.Errors.CompileError: surprise/prediction_algorithms/co_clustering.pyx\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.6.3)\n",
      "Collecting scikit-surprise\n",
      "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas scikit-learn nltk scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "# If you already have the dataset, you can skip or comment this step\n",
    "!kaggle datasets download -d hsankesara/medium-articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 337 articles\n"
     ]
    }
   ],
   "source": [
    "# Import and Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df = pd.read_csv('dataset/articles.csv')\n",
    "columns = ['author', 'claps', 'reading_time', 'link', 'title', 'text']\n",
    "df = df[columns]\n",
    "df = df.dropna()\n",
    "\n",
    "df['claps'] = pd.to_numeric(df['claps'], errors='coerce').fillna(0).astype(int)\n",
    "print(f\"Dataset loaded with {len(df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (337, 6)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "available_columns = ['author', 'claps', 'reading_time', 'link', 'title', 'text']  \n",
    "df = df[available_columns]\n",
    "df = df.dropna()\n",
    "print(\"Dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation Function\n",
    "def get_recommendations(topic, n=2):\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "    \n",
    "    query_vec = tfidf.transform([topic])\n",
    "    similarity = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get top matches and sort by claps\n",
    "    indices = similarity.argsort()[-n*2:][::-1]  \n",
    "    candidates = df.iloc[indices]\n",
    "    return candidates.nlargest(n, 'claps')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Interface\n",
    "def ask_for_recommendations():\n",
    "    topic = input(\"What topic are you interested in? \")\n",
    "    print(f\"\\nFinding articles about '{topic}'...\\n\")\n",
    "    \n",
    "    try:\n",
    "        recommendations = get_recommendations(topic)\n",
    "        print(\"Recommended Articles:\")\n",
    "        for _, article in recommendations.iterrows():\n",
    "            print(f\"\\nTitle: {article['title']}\")\n",
    "            print(f\"Author: {article['author']}\")\n",
    "            print(f\"Reading time: {article['reading_time']} mins\")\n",
    "            print(f\"Claps: {article['claps']}\")\n",
    "            print(f\"Link: {article['link']}\")\n",
    "            print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding articles about 'data'...\n",
      "\n",
      "Error: Column 'claps' has dtype object, cannot use method 'nlargest' with this dtype\n"
     ]
    }
   ],
   "source": [
    "# Run System\n",
    "ask_for_recommendations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
